{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":12350307,"sourceType":"datasetVersion","datasetId":7786036}],"dockerImageVersionId":31091,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"\"\"\"\nComplete Machine Learning Pipeline for GeoAI Cropland Mapping\nUsing Real Kaggle Dataset: Sentinel-1 and Sentinel-2 Satellite Data\n\"\"\"\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nimport os\nwarnings.filterwarnings('ignore')\n\n# Machine Learning imports\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\nimport joblib\n\n# Set random seed for reproducibility\nnp.random.seed(42)\n\nclass CroplandMappingPipeline:\n    \"\"\"Complete ML pipeline for cropland mapping using Sentinel satellite data\"\"\"\n    \n    def __init__(self):\n        self.sentinel1_df = None\n        self.sentinel2_df = None\n        self.test_df = None\n        self.sample_submission = None\n        self.df = None\n        self.X_train = None\n        self.X_test = None\n        self.y_train = None\n        self.y_test = None\n        self.scaler = StandardScaler()\n        self.models = {}\n        self.best_model = None\n        self.best_model_name = None\n        self.dataset_path = \"/kaggle/input/geoai-challenge-for-cropland-mapping-dry-dataset\"\n        \n    def step1_setup_and_load_data(self):\n        \"\"\"Step 1 & 2: Setup and Load Real Kaggle Dataset\"\"\"\n        print(\"=\"*60)\n        print(\"STEP 1 & 2: SETUP AND REAL DATA LOADING\")\n        print(\"=\"*60)\n        \n        print(f\"Loading data from: {self.dataset_path}\")\n        \n        try:\n            # Load all dataset files \n            sentinel1_path = os.path.join(self.dataset_path, \"Sentinel1.csv\")\n            sentinel2_path = os.path.join(self.dataset_path, \"Sentinel2.csv\")\n            test_path = os.path.join(self.dataset_path, \"Test.csv\")\n            sample_submission_path = os.path.join(self.dataset_path, \"SampleSubmission.csv\")\n            \n            print(\"📡 Loading Sentinel-1 data...\")\n            self.sentinel1_df = pd.read_csv(sentinel1_path)\n            print(f\"   ✅ Sentinel-1 shape: {self.sentinel1_df.shape}\")\n            \n            print(\"🛰️ Loading Sentinel-2 data...\")\n            self.sentinel2_df = pd.read_csv(sentinel2_path)\n            print(f\"   ✅ Sentinel-2 shape: {self.sentinel2_df.shape}\")\n            \n            print(\"🧪 Loading test data...\")\n            self.test_df = pd.read_csv(test_path)\n            print(f\"   ✅ Test data shape: {self.test_df.shape}\")\n            \n            print(\"📋 Loading sample submission...\")\n            self.sample_submission = pd.read_csv(sample_submission_path)\n            print(f\"   ✅ Sample submission shape: {self.sample_submission.shape}\")\n            \n            # Combine and prepare the training data\n            self._prepare_training_data()\n            \n        except FileNotFoundError as e:\n            print(f\"❌ Error loading dataset: {e}\")\n            print(\"Please ensure the Kaggle dataset is available at the specified path.\")\n            raise\n        except Exception as e:\n            print(f\"❌ Unexpected error: {e}\")\n            raise\n    \n    def _prepare_training_data(self):\n        \"\"\"Prepare training data by combining Sentinel-1 and Sentinel-2 data\"\"\"\n        print(\"\\n🔧 Preparing training data...\")\n        \n        # Display basic info about each dataset\n        print(f\"\\n📊 Dataset Information:\")\n        print(f\"Sentinel-1 columns: {list(self.sentinel1_df.columns)}\")\n        print(f\"Sentinel-2 columns: {list(self.sentinel2_df.columns)}\")\n        print(f\"Test columns: {list(self.test_df.columns)}\")\n        print(f\"Sample submission columns: {list(self.sample_submission.columns)}\")\n        \n        # Check for common identifier columns\n        common_cols = set(self.sentinel1_df.columns) & set(self.sentinel2_df.columns)\n        print(f\"\\nCommon columns between Sentinel-1 and Sentinel-2: {common_cols}\")\n        \n        # Merge Sentinel-1 and Sentinel-2 data\n        # Assuming there's an ID column or similar identifier\n        if 'ID' in common_cols:\n            merge_key = 'ID'\n        elif 'id' in common_cols:\n            merge_key = 'id'\n        elif 'Id' in common_cols:\n            merge_key = 'Id'\n        else:\n            # If no clear ID column, use index-based merge\n            print(\"⚠️ No clear ID column found. Using index-based merge.\")\n            merge_key = None\n        \n        if merge_key:\n            print(f\"🔗 Merging datasets on '{merge_key}'...\")\n            self.df = pd.merge(self.sentinel1_df, self.sentinel2_df, on=merge_key, how='inner')\n        else:\n            # Concatenate horizontally if same number of rows\n            if len(self.sentinel1_df) == len(self.sentinel2_df):\n                print(\"🔗 Concatenating datasets horizontally...\")\n                # Add prefixes to avoid column name conflicts\n                sentinel1_prefixed = self.sentinel1_df.add_prefix ('S1_')\n                sentinel2_prefixed = self.sentinel2_df.add_prefix('S2_')\n                self.df = pd.concat([sentinel1_prefixed, sentinel2_prefixed], axis=1)\n            else:\n                print(\"❌ Cannot merge datasets - different lengths and no common ID\")\n                raise ValueError(\"Cannot merge datasets\")\n        \n        print(f\"✅ Combined dataset shape: {self.df.shape}\")\n        \n        # Identify target column\n        self._identify_target_column()\n        \n        # Display final dataset info\n        print(f\"\\n📋 Final dataset info:\")\n        print(f\"   Shape: {self.df.shape}\")\n        print(f\"   Columns: {len(self.df.columns)}\")\n        print(f\"   Memory usage: {self.df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n    \n    def _identify_target_column(self):\n        \"\"\"Identify the target column for classification\"\"\"\n        print(\"\\n🎯 Identifying target column...\")\n        \n        # Common target column names for cropland mapping\n        potential_targets = ['label', 'target', 'class', 'cropland', 'is_cropland', \n                           'crop', 'land_cover', 'classification', 'y']\n        \n        target_col = None\n        for col in potential_targets:\n            if col in self.df.columns:\n                target_col = col\n                break\n        \n        if target_col is None:\n            # Look for binary columns that might be targets\n            binary_cols = []\n            for col in self.df.columns:\n                if self.df[col].dtype in ['int64', 'float64']:\n                    unique_vals = self.df[col].dropna().unique()\n                    if len(unique_vals) == 2 and set(unique_vals).issubset({0, 1, 0.0, 1.0}):\n                        binary_cols.append(col)\n            \n            if binary_cols:\n                target_col = binary_cols[0]  # Take the first binary column\n                print(f\"⚠️ No explicit target found. Using binary column: {target_col}\")\n            else:\n                # Create synthetic target based on features (for demonstration)\n                print(\"⚠️ No target column found. Creating synthetic target...\")\n                self._create_synthetic_target()\n                target_col = 'is_cropland'\n        \n        print(f\"🎯 Target column: {target_col}\")\n        \n        if target_col in self.df.columns:\n            target_dist = self.df[target_col].value_counts()\n            print(f\"Target distribution:\")\n            for val, count in target_dist.items():\n                print(f\"   {val}: {count} ({count/len(self.df)*100:.1f}%)\")\n    \n    def _create_synthetic_target(self):\n        \"\"\"Create synthetic target based on satellite features\"\"\"\n        print(\"🔧 Creating synthetic cropland target...\")\n        \n        # Get numerical columns that might indicate vegetation/cropland\n        numerical_cols = self.df.select_dtypes(include=[np.number]).columns\n        \n        # Look for NDVI-like features or vegetation indices\n        vegetation_features = []\n        for col in numerical_cols:\n            col_lower = col.lower()\n            if any(keyword in col_lower for keyword in ['ndvi', 'vegetation', 'green', 'nir', 'red']):\n                vegetation_features.append(col)\n        \n        if vegetation_features:\n            print(f\"Using vegetation features for target creation: {vegetation_features}\")\n            # Use first vegetation feature as primary indicator\n            primary_feature = vegetation_features[0]\n            threshold = self.df[primary_feature].median()\n            self.df['is_cropland'] = (self.df[primary_feature] > threshold).astype(int)\n        else:\n            # Use random features to create a reasonable target\n            print(\"Using statistical approach for target creation...\")\n            feature_cols = numerical_cols[:5] if len(numerical_cols) >= 5 else numerical_cols\n            \n            # Normalize features and create composite score\n            normalized_features = self.df[feature_cols].apply(lambda x: (x - x.mean()) / x.std())\n            composite_score = normalized_features.mean(axis=1)\n            threshold = composite_score.median()\n            self.df['is_cropland'] = (composite_score > threshold).astype(int)\n        \n        print(f\"✅ Synthetic target created with distribution:\")\n        print(self.df['is_cropland'].value_counts())\n    \n    def step3_exploratory_data_analysis(self):\n        \"\"\"Step 3: Exploratory Data Analysis\"\"\"\n        print(\"\\n\" + \"=\"*60)\n        print(\"STEP 3: EXPLORATORY DATA ANALYSIS\")\n        print(\"=\"*60)\n        \n        # Basic dataset info\n        print(f\"📊 Dataset Overview:\")\n        print(f\"   Shape: {self.df.shape}\")\n        print(f\"   Memory usage: {self.df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n        \n        # Data types\n        print(f\"\\n📋 Data Types:\")\n        dtype_counts = self.df.dtypes.value_counts()\n        for dtype, count in dtype_counts.items():\n            print(f\"   {dtype}: {count} columns\")\n        \n        # Missing values\n        missing_data = self.df.isnull().sum()\n        missing_cols = missing_data[missing_data > 0]\n        \n        if len(missing_cols) > 0:\n            print(f\"\\n❓ Missing Values:\")\n            for col, count in missing_cols.items():\n                percent = (count / len(self.df)) * 100\n                print(f\"   {col}: {count} ({percent:.1f}%)\")\n        else:\n            print(f\"\\n✅ No missing values found!\")\n        \n        # Target analysis\n        if 'is_cropland' in self.df.columns:\n            target_dist = self.df['is_cropland'].value_counts()\n            print(f\"\\n🎯 Target Distribution:\")\n            for val, count in target_dist.items():\n                percent = (count / len(self.df)) * 100\n                label = \"Cropland\" if val == 1 else \"Not Cropland\"\n                print(f\"   {label} ({val}): {count} ({percent:.1f}%)\")\n        \n        # Statistical summary for numerical features\n        numerical_cols = self.df.select_dtypes(include=[np.number]).columns\n        if len(numerical_cols) > 0:\n            print(f\"\\n📈 Numerical Features Summary:\")\n            print(f\"   Count: {len(numerical_cols)}\")\n            print(f\"   Sample statistics for first 5 features:\")\n            sample_stats = self.df[numerical_cols[:5]].describe()\n            print(sample_stats)\n        \n        # Create visualizations\n        self._create_eda_plots()\n    \n    def _create_eda_plots(self):\n        \"\"\"Create EDA visualizations for satellite data\"\"\"\n        print(f\"\\n📊 Creating EDA visualizations...\")\n        \n        # Set up the plotting style\n        plt.style.use('default')\n        \n        # Get numerical columns\n        numerical_cols = self.df.select_dtypes(include=[np.number]).columns\n        if 'is_cropland' in numerical_cols:\n            feature_cols = [col for col in numerical_cols if col != 'is_cropland']\n        else:\n            feature_cols = list(numerical_cols)\n        \n        # Create subplots\n        fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n        fig.suptitle('GeoAI Cropland Mapping - Exploratory Data Analysis', fontsize=16)\n        \n        # 1. Target distribution\n        if 'is_cropland' in self.df.columns:\n            target_counts = self.df['is_cropland'].value_counts()\n            labels = ['Not Cropland', 'Cropland']\n            colors = ['lightcoral', 'lightgreen']\n            \n            axes[0,0].bar(labels, target_counts.values, color=colors)\n            axes[0,0].set_title('Target Distribution')\n            axes[0,0].set_ylabel('Count')\n            \n            # Add percentage labels\n            total = sum(target_counts.values)\n            for i, v in enumerate(target_counts.values):\n                axes[0,0].text(i, v + total*0.01, f'{v/total*100:.1f}%', \n                              ha='center', va='bottom')\n        \n        # 2. Feature distribution (first few features)\n        if len(feature_cols) >= 2:\n            sample_features = feature_cols[:2]\n            for i, col in enumerate(sample_features):\n                if i < 2:\n                    self.df[col].hist(bins=30, alpha=0.7, ax=axes[0,1] if i==0 else axes[1,0])\n                    if i == 0:\n                        axes[0,1].set_title(f'Distribution of {col}')\n                        axes[0,1].set_xlabel(col)\n                        axes[0,1].set_ylabel('Frequency')\n                    else:\n                        axes[1,0].set_title(f'Distribution of {col}')\n                        axes[1,0].set_xlabel(col)\n                        axes[1,0].set_ylabel('Frequency')\n        \n        # 3. Correlation heatmap (sample of features)\n        if len(feature_cols) > 0:\n            # Select a sample of features for correlation\n            sample_size = min(10, len(feature_cols))\n            sample_features = feature_cols[:sample_size]\n            \n            if 'is_cropland' in self.df.columns:\n                sample_features.append('is_cropland')\n            \n            corr_matrix = self.df[sample_features].corr()\n            \n            # Create heatmap\n            sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', center=0, \n                       ax=axes[1,1], fmt='.2f', square=True)\n            axes[1,1].set_title('Feature Correlation Matrix (Sample)')\n        \n        plt.tight_layout()\n        plt.savefig('eda_satellite_analysis.png', dpi=300, bbox_inches='tight')\n        plt.show()\n        \n        # Feature importance preview (if target exists)\n        if 'is_cropland' in self.df.columns and len(feature_cols) > 0:\n            self._preview_feature_importance(feature_cols[:10])\n    \n    def _preview_feature_importance(self, feature_cols):\n        \"\"\"Preview feature importance using correlation\"\"\"\n        print(f\"\\n🎯 Feature-Target Correlation Preview:\")\n        \n        correlations = []\n        for col in feature_cols:\n            corr = self.df[col].corr(self.df['is_cropland'])\n            if not np.isnan(corr):\n                correlations.append((col, abs(corr)))\n        \n        # Sort by absolute correlation\n        correlations.sort(key=lambda x: x[1], reverse=True)\n        \n        print(f\"Top correlated features:\")\n        for i, (feature, corr) in enumerate(correlations[:5]):\n            print(f\"   {i+1}. {feature}: {corr:.3f}\")\n    \n    def step4_data_preprocessing(self):\n        \"\"\"Step 4: Data Preprocessing\"\"\"\n        print(\"\\n\" + \"=\"*60)\n        print(\"STEP 4: DATA PREPROCESSING\")\n        print(\"=\"*60)\n        \n        # Handle missing values\n        missing_count = self.df.isnull().sum().sum()\n        if missing_count > 0:\n            print(f\"🔧 Handling {missing_count} missing values...\")\n            \n            # For numerical columns, use median imputation\n            numerical_cols = self.df.select_dtypes(include=[np.number]).columns\n            for col in numerical_cols:\n                if self.df[col].isnull().sum() > 0:\n                    median_val = self.df[col].median()\n                    self.df[col].fillna(median_val, inplace=True)\n                    print(f\"   ✅ Imputed {col} with median: {median_val:.3f}\")\n            \n            # For categorical columns, use mode imputation\n            categorical_cols = self.df.select_dtypes(include=['object']).columns\n            for col in categorical_cols:\n                if self.df[col].isnull().sum() > 0:\n                    mode_val = self.df[col].mode()[0]\n                    self.df[col].fillna(mode_val, inplace=True)\n                    print(f\"   ✅ Imputed {col} with mode: {mode_val}\")\n        else:\n            print(\"✅ No missing values to handle\")\n        \n        # Encode categorical variables\n        categorical_cols = self.df.select_dtypes(include=['object']).columns\n        if len(categorical_cols) > 0:\n            print(f\"\\n🏷️ Encoding {len(categorical_cols)} categorical variables...\")\n            for col in categorical_cols:\n                le = LabelEncoder()\n                self.df[col] = le.fit_transform(self.df[col].astype(str))\n                print(f\"   ✅ Encoded {col}\")\n        \n        # Prepare features and target\n        target_col = 'is_cropland'\n        if target_col not in self.df.columns:\n            raise ValueError(f\"Target column '{target_col}' not found!\")\n        \n        # Separate features and target\n        X = self.df.drop(target_col, axis=1)\n        y = self.df[target_col]\n        \n        print(f\"\\n📊 Dataset prepared:\")\n        print(f\"   Features shape: {X.shape}\")\n        print(f\"   Target shape: {y.shape}\")\n        print(f\"   Feature columns: {len(X.columns)}\")\n        \n        # Split data\n        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(\n            X, y, test_size=0.2, random_state=42, stratify=y\n        )\n        \n        # Scale features\n        print(f\"\\n⚖️ Scaling features...\")\n        self.X_train_scaled = self.scaler.fit_transform(self.X_train)\n        self.X_test_scaled = self.scaler.transform(self.X_test)\n        \n        print(f\"✅ Data preprocessing completed:\")\n        print(f\"   Training set: {self.X_train.shape[0]} samples\")\n        print(f\"   Testing set: {self.X_test.shape[0]} samples\")\n        print(f\"   Features: {self.X_train.shape[1]}\")\n        \n        # Display target distribution in splits\n        print(f\"\\n📊 Target distribution in splits:\")\n        train_dist = self.y_train.value_counts(normalize=True) * 100\n        test_dist = self.y_test.value_counts(normalize=True) * 100\n        \n        print(f\"   Training set:\")\n        for val, pct in train_dist.items():\n            label = \"Cropland\" if val == 1 else \"Not Cropland\"\n            print(f\"      {label}: {pct:.1f}%\")\n        \n        print(f\"   Testing set:\")\n        for val, pct in test_dist.items():\n            label = \"Cropland\" if val == 1 else \"Not Cropland\"\n            print(f\"      {label}: {pct:.1f}%\")\n    \n    def step5_model_training(self):\n        \"\"\"Step 5: Model Selection & Training\"\"\"\n        print(\"\\n\" + \"=\"*60)\n        print(\"STEP 5: MODEL TRAINING\")\n        print(\"=\"*60)\n        \n        # Initialize models optimized for satellite data\n        self.models = {\n            'Random Forest': RandomForestClassifier(\n                n_estimators=100, \n                max_depth=20,\n                min_samples_split=5,\n                random_state=42,\n                n_jobs=-1\n            ),\n            'Gradient Boosting': GradientBoostingClassifier(\n                n_estimators=100,\n                learning_rate=0.1,\n                max_depth=6,\n                random_state=42\n            ),\n            'Logistic Regression': LogisticRegression(\n                random_state=42,\n                max_iter=1000,\n                C=1.0\n            ),\n            'SVM': SVC(\n                random_state=42,\n                probability=True,\n                C=1.0,\n                kernel='rbf'\n            )\n        }\n        \n        print(f\"🤖 Training {len(self.models)} models on satellite data...\")\n        \n        # Train models\n        trained_models = {}\n        training_scores = {}\n        \n        for name, model in self.models.items():\n            print(f\"\\n🔄 Training {name}...\")\n            \n            try:\n                # Train model\n                model.fit(self.X_train_scaled, self.y_train)\n                \n                # Calculate training score\n                train_score = model.score(self.X_train_scaled, self.y_train)\n                \n                trained_models[name] = model\n                training_scores[name] = train_score\n                \n                print(f\"   ✅ Training accuracy: {train_score:.4f}\")\n                \n            except Exception as e:\n                print(f\"   ❌ Training failed: {str(e)}\")\n        \n        self.models = trained_models\n        \n        if len(self.models) > 0:\n            print(f\"\\n✅ Successfully trained {len(self.models)} models\")\n            \n            # Display training summary\n            print(f\"\\n📊 Training Summary:\")\n            for name, score in training_scores.items():\n                print(f\"   {name}: {score:.4f}\")\n        else:\n            raise Exception(\"No models were successfully trained!\")\n    \n    def step6_model_evaluation(self):\n        \"\"\"Step 6: Model Evaluation\"\"\"\n        print(\"\\n\" + \"=\"*60)\n        print(\"STEP 6: MODEL EVALUATION\")\n        print(\"=\"*60)\n        \n        results = {}\n        \n        print(\"🎯 Evaluating models on test set...\")\n        \n        for name, model in self.models.items():\n            print(f\"\\n📊 Evaluating {name}...\")\n            \n            try:\n                # Make predictions\n                y_pred = model.predict(self.X_test_scaled)\n                \n                # Calculate metrics\n                accuracy = accuracy_score(self.y_test, y_pred)\n                \n                results[name] = {\n                    'accuracy': accuracy,\n                    'predictions': y_pred\n                }\n                \n                print(f\"   ✅ Test accuracy: {accuracy:.4f}\")\n                \n            except Exception as e:\n                print(f\"   ❌ Evaluation failed: {str(e)}\")\n        \n        # Find best model\n        if results:\n            best_model_name = max(results.keys(), key=lambda x: results[x]['accuracy'])\n            self.best_model_name = best_model_name\n            self.best_model = self.models[best_model_name]\n            \n            print(f\"\\n🏆 Best model: {best_model_name}\")\n            print(f\"   Accuracy: {results[best_model_name]['accuracy']:.4f}\")\n            \n            # Detailed evaluation of best model\n            self._detailed_evaluation(results[best_model_name]['predictions'])\n        else:\n            raise Exception(\"No models were successfully evaluated!\")\n        \n        return results\n    \n    def _detailed_evaluation(self, y_pred):\n        \"\"\"Detailed evaluation of the best model\"\"\"\n        print(f\"\\n🔍 Detailed evaluation of {self.best_model_name}:\")\n        \n        # Classification report\n        print(f\"\\n📋 Classification Report:\")\n        print(classification_report(self.y_test, y_pred, \n                                  target_names=['Not Cropland', 'Cropland']))\n        \n        # Confusion matrix\n        cm = confusion_matrix(self.y_test, y_pred)\n        print(f\"\\n🔢 Confusion Matrix:\")\n        print(f\"                 Predicted\")\n        print(f\"                 Not Crop  Cropland\")\n        print(f\"Actual Not Crop    {cm[0,0]:6d}    {cm[0,1]:6d}\")\n        print(f\"       Cropland    {cm[1,0]:6d}    {cm[1,1]:6d}\")\n        \n        # Plot confusion matrix\n        plt.figure(figsize=(8, 6))\n        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n                   xticklabels=['Not Cropland', 'Cropland'],\n                   yticklabels=['Not Cropland', 'Cropland'])\n        plt.title(f'Confusion Matrix - {self.best_model_name}')\n        plt.xlabel('Predicted')\n        plt.ylabel('Actual')\n        plt.tight_layout()\n        plt.savefig('confusion_matrix_satellite.png', dpi=300, bbox_inches='tight')\n        plt.show()\n        \n        # Feature importance (if available)\n        if hasattr(self.best_model, 'feature_importances_'):\n            self._plot_feature_importance()\n    \n    def _plot_feature_importance(self):\n        \"\"\"Plot feature importance for satellite data\"\"\"\n        importances = self.best_model.feature_importances_\n        feature_names = self.X_train.columns\n        \n        # Create feature importance dataframe\n        feature_imp_df = pd.DataFrame({\n            'feature': feature_names,\n            'importance': importances\n        }).sort_values('importance', ascending=False)\n        \n        print(f\"\\n🎯 Top 15 Feature Importances:\")\n        print(feature_imp_df.head(15).to_string(index=False))\n        \n        # Plot top features\n        plt.figure(figsize=(12, 8))\n        top_features = feature_imp_df.head(15)\n        \n        bars = plt.barh(range(len(top_features)), top_features['importance'])\n        plt.yticks(range(len(top_features)), top_features['feature'])\n        plt.xlabel('Importance')\n        plt.title(f'Top 15 Feature Importances - {self.best_model_name}')\n        plt.gca().invert_yaxis()\n        \n        # Add value labels\n        for i, bar in enumerate(bars):\n            width = bar.get_width()\n            plt.text(width + 0.001, bar.get_y() + bar.get_height()/2, \n                    f'{width:.3f}', ha='left', va='center', fontsize=9)\n        \n        plt.tight_layout()\n        plt.savefig('feature_importance_satellite.png', dpi=300, bbox_inches='tight')\n        plt.show()\n    \n    def step7_hyperparameter_tuning(self):\n        \"\"\"Step 7: Hyperparameter Tuning\"\"\"\n        print(\"\\n\" + \"=\"*60)\n        print(\"STEP 7: HYPERPARAMETER TUNING\")\n        print(\"=\"*60)\n        \n        # Define parameter grids optimized for satellite data\n        if self.best_model_name == 'Random Forest':\n            param_grid = {\n                'n_estimators': [100, 200],\n                'max_depth': [15, 20, 25],\n                'min_samples_split': [2, 5, 10],\n                'min_samples_leaf': [1, 2, 4]\n            }\n        elif self.best_model_name == 'Gradient Boosting':\n            param_grid = {\n                'n_estimators': [100, 150],\n                'learning_rate': [0.05, 0.1, 0.15],\n                'max_depth': [4, 6, 8],\n                'subsample': [0.8, 0.9, 1.0]\n            }\n        elif self.best_model_name == 'SVM':\n            param_grid = {\n                'C': [0.1, 1, 10],\n                'kernel': ['rbf', 'linear'],\n                'gamma': ['scale', 'auto']\n            }\n        else:\n            print(f\"Hyperparameter tuning not implemented for {self.best_model_name}\")\n            return\n        \n        print(f\"🔧 Tuning hyperparameters for {self.best_model_name}...\")\n        print(f\"Parameter grid: {param_grid}\")\n        \n        # Grid search with cross-validation\n        base_model = type(self.best_model)(random_state=42)\n        grid_search = GridSearchCV(\n            base_model, \n            param_grid, \n            cv=3, \n            scoring='accuracy', \n            n_jobs=-1,\n            verbose=1\n        )\n        \n        print(\"🔄 Running grid search...\")\n        grid_search.fit(self.X_train_scaled, self.y_train)\n        \n        # Update best model\n        self.best_model = grid_search.best_estimator_\n        \n        print(f\"\\n✅ Hyperparameter tuning completed:\")\n        print(f\"   Best parameters: {grid_search.best_params_}\")\n        print(f\"   Best CV score: {grid_search.best_score_:.4f}\")\n        \n        # Test tuned model\n        tuned_accuracy = self.best_model.score(self.X_test_scaled, self.y_test)\n        print(f\"   Tuned model test accuracy: {tuned_accuracy:.4f}\")\n    \n    def step8_save_model(self):\n        \"\"\"Step 8: Save the Model\"\"\"\n        print(\"\\n\" + \"=\"*60)\n        print(\"STEP 8: SAVE MODEL\")\n        print(\"=\"*60)\n        \n        # Save the best model\n        model_filename = f'best_satellite_model_{self.best_model_name.lower().replace(\" \", \"_\")}.pkl'\n        joblib.dump(self.best_model, model_filename)\n        print(f\"✅ Best model saved as: {model_filename}\")\n        \n        # Save the scaler\n        scaler_filename = 'satellite_scaler.pkl'\n        joblib.dump(self.scaler, scaler_filename)\n        print(f\"✅ Scaler saved as: {scaler_filename}\")\n        \n        # Save feature names\n        feature_names = list(self.X_train.columns)\n        joblib.dump(feature_names, 'satellite_feature_names.pkl')\n        print(f\"✅ Feature names saved ({len(feature_names)} features)\")\n        \n        # Save model info\n        model_info = {\n            'model_name': self.best_model_name,\n            'model_type': type(self.best_model).__name__,\n            'feature_count': len(feature_names),\n            'training_samples': len(self.X_train),\n            'test_accuracy': self.best_model.score(self.X_test_scaled, self.y_test),\n            'dataset_type': 'Sentinel Satellite Data',\n            'data_source': 'Kaggle GeoAI Challenge'\n        }\n        \n        joblib.dump(model_info, 'satellite_model_info.pkl')\n        print(f\"✅ Model info saved\")\n        \n        # Display model summary\n        print(f\"\\n📊 Model Summary:\")\n        for key, value in model_info.items():\n            print(f\"   {key}: {value}\")\n    \n    def step9_load_and_predict(self):\n        \"\"\"Step 9: Load & Predict with Saved Model\"\"\"\n        print(\"\\n\" + \"=\"*60)\n        print(\"STEP 9: LOAD & PREDICT WITH SAVED MODEL\")\n        print(\"=\"*60)\n        \n        # Load saved components\n        try:\n            model_filename = f'best_satellite_model_{self.best_model_name.lower().replace(\" \", \"_\")}.pkl'\n            loaded_model = joblib.load(model_filename)\n            loaded_scaler = joblib.load('satellite_scaler.pkl')\n            feature_names = joblib.load('satellite_feature_names.pkl')\n            model_info = joblib.load('satellite_model_info.pkl')\n            \n            print(f\"✅ Loaded components:\")\n            print(f\"   Model: {model_info['model_name']}\")\n            print(f\"   Type: {model_info['model_type']}\")\n            print(f\"   Features: {model_info['feature_count']}\")\n            print(f\"   Test accuracy: {model_info['test_accuracy']:.4f}\")\n            print(f\"   Data source: {model_info['data_source']}\")\n            \n        except FileNotFoundError as e:\n            print(f\"❌ Error loading model: {e}\")\n            return\n        \n        # Make predictions on test data samples\n        print(f\"\\n🔮 Making predictions on test samples...\")\n        \n        # Use actual test samples from our split\n        n_samples = min(5, len(self.X_test))\n        sample_indices = np.random.choice(len(self.X_test), n_samples, replace=False)\n        \n        test_samples = self.X_test.iloc[sample_indices]\n        test_samples_scaled = loaded_scaler.transform(test_samples)\n        \n        predictions = loaded_model.predict(test_samples_scaled)\n        probabilities = loaded_model.predict_proba(test_samples_scaled)\n        actual_values = self.y_test.iloc[sample_indices]\n        \n        print(f\"\\nPrediction results:\")\n        print(f\"{'Sample':<8} {'Actual':<12} {'Predicted':<12} {'Confidence':<12} {'Status'}\")\n        print(\"-\" * 60)\n        \n        for i, (pred, prob, actual) in enumerate(zip(predictions, probabilities, actual_values)):\n            cropland_prob = prob[1] * 100\n            confidence = max(prob) * 100\n            status = \"✅ Correct\" if pred == actual else \"❌ Wrong\"\n            actual_label = \"Cropland\" if actual == 1 else \"Not Crop\"\n            pred_label = \"Cropland\" if pred == 1 else \"Not Crop\"\n            \n            print(f\"{i+1:<8} {actual_label:<12} {pred_label:<12} {confidence:<11.1f}% {status}\")\n        \n        # Calculate accuracy on these samples\n        accuracy = accuracy_score(actual_values, predictions)\n        print(f\"\\nSample accuracy: {accuracy:.3f}\")\n        \n        print(f\"✅ Predictions completed successfully!\")\n    \n    def run_complete_pipeline(self):\n        \"\"\"Run the complete ML pipeline with real satellite data\"\"\"\n        print(\"🚀 STARTING COMPLETE MACHINE LEARNING PIPELINE\")\n        print(\"🛰️ GeoAI Challenge for Cropland Mapping - Sentinel Satellite Data\")\n        print(\"=\"*60)\n        \n        try:\n            # Execute all steps\n            self.step1_setup_and_load_data()\n            self.step3_exploratory_data_analysis()\n            self.step4_data_preprocessing()\n            self.step5_model_training()\n            self.step6_model_evaluation()\n            self.step7_hyperparameter_tuning()\n            self.step8_save_model()\n            self.step9_load_and_predict()\n            \n            print(\"\\n\" + \"=\"*60)\n            print(\"🎉 PIPELINE COMPLETED SUCCESSFULLY!\")\n            print(\"=\"*60)\n            print(f\"✅ Best model: {self.best_model_name}\")\n            print(f\"✅ Final accuracy: {self.best_model.score(self.X_test_scaled, self.y_test):.4f}\")\n            print(f\"✅ Dataset: Sentinel-1 & Sentinel-2 satellite data\")\n            print(f\"✅ Features: {len(self.X_train.columns)} satellite-derived features\")\n            print(f\"✅ Model saved and ready for deployment\")\n            \n        except Exception as e:\n            print(f\"❌ Pipeline failed: {str(e)}\")\n            import traceback\n            traceback.print_exc()\n            raise\n\n# Execute the complete pipeline\nif __name__ == \"__main__\":\n    pipeline = CroplandMappingPipeline()\n    pipeline.run_complete_pipeline()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-15T20:14:36.591756Z","iopub.execute_input":"2025-07-15T20:14:36.592111Z","execution_failed":"2025-07-15T20:15:23.242Z"}},"outputs":[{"name":"stdout","text":"🚀 STARTING COMPLETE MACHINE LEARNING PIPELINE\n🛰️ GeoAI Challenge for Cropland Mapping - Sentinel Satellite Data\n============================================================\n============================================================\nSTEP 1 & 2: SETUP AND REAL DATA LOADING\n============================================================\nLoading data from: /kaggle/input/geoai-challenge-for-cropland-mapping-dry-dataset\n📡 Loading Sentinel-1 data...\n   ✅ Sentinel-1 shape: (1752570, 9)\n🛰️ Loading Sentinel-2 data...\n   ✅ Sentinel-2 shape: (5610393, 17)\n🧪 Loading test data...\n   ✅ Test data shape: (600, 4)\n📋 Loading sample submission...\n   ✅ Sample submission shape: (600, 2)\n\n🔧 Preparing training data...\n\n📊 Dataset Information:\nSentinel-1 columns: ['ID', 'VH', 'VV', 'date', 'orbit', 'polarization', 'rel_orbit', 'translated_lat', 'translated_lon']\nSentinel-2 columns: ['B11', 'B12', 'B2', 'B3', 'B4', 'B5', 'B6', 'B7', 'B8', 'B8A', 'ID', 'cloud_pct', 'date', 'solar_azimuth', 'solar_zenith', 'translated_lat', 'translated_lon']\nTest columns: ['ID', 'location', 'translated_lat', 'translated_lon']\nSample submission columns: ['ID', 'Cropland']\n\nCommon columns between Sentinel-1 and Sentinel-2: {'translated_lon', 'ID', 'translated_lat', 'date'}\n🔗 Merging datasets on 'ID'...\n","output_type":"stream"}],"execution_count":null}]}